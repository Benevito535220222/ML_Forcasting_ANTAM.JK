# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sVyEEqhcnw-SQY3XMpFaYV1Ekq-HAg3l
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, GRU
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import RandomizedSearchCV

"""### EDA

Tahap 1: Mengimpor dataset. Pada tahap ini, dataset saham `ANTM.JK.csv` dimuat menggunakan `pandas`. Kolom "Date" juga diparsing menjadi format tanggal agar dapat dianalisis secara temporal di tahap selanjutnya. Selanjutnya menyortir data berdasarkan tanggal. Langkah ini menyortir data berdasarkan kolom `Date` secara ascending, untuk memastikan urutan kronologis saat melakukan analisis time series
"""

df = pd.read_csv("ANTM.JK.csv", parse_dates=["Date"])
df.sort_values("Date", inplace=True)

"""Tahap 2: Menampilkan informasi awal data. Digunakan fungsi `.info()` untuk melihat struktur dataset, seperti jumlah baris, kolom, tipe data, dan jumlah nilai non-null di setiap kolom"""

df.info()

"""Tahap 3: Statistik deskriptif data. Menampilkan ringkasan statistik seperti mean, standar deviasi, nilai minimum, maksimum, dan kuartil dari setiap kolom numerik"""

df.describe()

"""Tahap 4: Mengecek nilai yang hilang. Langkah ini penting untuk mengetahui apakah terdapat data yang hilang pada kolom-kolom dataset, yang dapat mempengaruhi kualitas analisis"""

df.isnull().sum()

"""Tahap 5: Pemeriksaan duplikasi data. Pada tahap ini dilakukan pengecekan apakah terdapat baris data yang terduplikasi dalam dataset. Duplikasi data dapat menyebabkan bias dalam analisis dan pelatihan model, sehingga penting untuk diidentifikasi dan dibersihkan jika perlu"""

df.duplicated().sum()

"""Tahap 6: Deteksi outlier. Pada tahap ini dilakukan deteksi outlier pada kolom harga penutupan (Close) menggunakan metode Interquartile Range (IQR). Outlier adalah data yang terletak jauh dari sebaran umum.

Langkah-langkah:

1. Dihitung nilai Q1 (kuartil bawah), Q3 (kuartil atas), dan IQR (Q3 - Q1).

2. Ditentukan batas bawah dan atas menggunakan rumus:
    - Lower Bound = Q1 - 1.5 * IQR
    - Upper Bound = Q3 + 1.5 * IQR

    Data yang berada di luar rentang tersebut dikategorikan sebagai outlier.

Visualisasi dilakukan menggunakan scatter plot berdasarkan waktu (Date) untuk menunjukkan persebaran harga Close dan menandai outlier dengan warna merah. Grafik menunjukan ada 4 periode dimana harga Close masuk ke bagian outlier diwaktu yang berdekatan. Dua di antaranya merupakan periode signifikan yang terjadi pada tahun 2007â€“2008 dan 2020, di mana harga saham menunjukkan lonjakan atau penurunan tajam yang menyebabkan data berada di luar batas wajar. Sementara itu, dua periode lainnya menunjukkan kemunculan outlier yang hanya berlangsung sesaat dan tidak berkelanjutan. Fenomena ini mengindikasikan adanya potensi kejadian ekstrem atau sentimen pasar yang kuat pada waktu-waktu tertentu
"""

# Hitung Q1, Q3 dan IQR
Q1 = df["Close"].quantile(0.30)
Q3 = df["Close"].quantile(0.70)
IQR = Q3 - Q1

# Tentukan batas bawah dan atas
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

# Deteksi outlier
outliers = df[(df["Close"] < lower) | (df["Close"] > upper)]
print("Jumlah outlier:", outliers.shape[0])

# Visualisasi sebaran outlier
plt.figure(figsize=(12, 6))
plt.scatter(df["Date"], df["Close"], label="Data", alpha=0.6)
plt.scatter(outliers["Date"], outliers["Close"], color="red", label="Outlier", s=50)
plt.title("Visualisasi Harga Close dengan Outlier")
plt.xlabel("Tanggal")
plt.ylabel("Harga Close")
plt.legend()
plt.tight_layout()
plt.show()

"""Tahap 7: Visualisasi harga close seiring waktu. Grafik garis berikut menampilkan tren harga penutupan (Close) saham ANTM.JK dari waktu ke waktu. Visualisasi ini berfungsi untuk mengidentifikasi pola musiman, tren jangka panjang, dan tingkat volatilitas harga. Dari grafik dapat diamati adanya lonjakan harga yang signifikan pada periode 2006 hingga 2008, yang dipengaruhi oleh dinamika pasar global akibat krisis ekonomi saat itu. Setelah periode tersebut, harga mengalami penurunan, sebelum akhirnya kembali meningkat secara tajam pada tahun 2020, yang berkorelasi dengan dampak pandemi COVID-19 terhadap pasar komoditas dan saham tambang. Tren ini mencerminkan bagaimana kondisi ekonomi makro memengaruhi pergerakan harga saham dalam jangka panjang"""

plt.figure(figsize=(12, 6))
plt.plot(df["Date"], df["Close"], label="Harga Close", color="blue")
plt.title("Harga Close Seiring Waktu")
plt.xlabel("Tanggal")
plt.ylabel("Harga")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""Tahap 7: Visualisasi distribusi harga close. Distribusi harga penutupan ("Close") divisualisasikan menggunakan histogram dan KDE untuk memahami sebarann harga dalam periode yang diamati. Terlihat dari grafik, harga close paling banyak berada dikisaran 500 - 1250. Hal ini mengindikasikan bahwa selama mayoritas waktu dalam rentang data, harga saham cenderung berada pada level tersebut. Informasi ini penting untuk memahami karakteristik umum pergerakan harga dan membantu dalam analisis risiko serta pengambilan keputusan investasi"""

plt.figure(figsize=(8, 4))
sns.histplot(df["Close"], bins=50, kde=True, color="skyblue")
plt.title("Distribusi Harga Close")
plt.xlabel("Harga Close")
plt.tight_layout()
plt.show()

"""Tahap 8: Visualisasi korelasi antar fitur. Matriks korelasi ditampilkan dalam bentuk heatmap untuk menunjukkan tingkat hubungan linier antar fitur numerik seperti Open, High, Low, Close, Adj Close, dan Volume. Visualisasi ini membantu dalam mengidentifikasi fitur-fitur yang saling berkaitan erat, yang penting untuk analisis multivariat maupun pemilihan fitur dalam pemodelan. Dari heatmap terlihat bahwa fitur-fitur harga (Open, High, Low, Close, dan Adj Close) memiliki korelasi sangat tinggi satu sama lain, berada di kisaran 0.98 hingga hampir 1.00, menandakan bahwa pergerakan satu harga hampir selalu diikuti oleh fitur harga lainnya. Sebaliknya, Volume menunjukkan korelasi yang rendah terhadap fitur lainnya, dengan nilai korelasi hanya berkisar antara 0.13 hingga 0.22, yang mengindikasikan bahwa jumlah transaksi tidak secara langsung sejalan dengan perubahan harga."""

plt.figure(figsize=(8, 6))
sns.heatmap(df[["Open", "High", "Low", "Close", "Adj Close", "Volume"]].corr(), annot=True, cmap="Blues")
plt.title("Matriks Korelasi Antar Fitur")
plt.show()

"""### Preprocessing

Tahap 9: Menghapus Baris dengan Nilai Kosong. Langkah ini menghapus seluruh baris yang memiliki nilai kosong ("NaN") agar tidak mengganggu proses pelatihan model dan menjaga integritas data
"""

df.dropna(inplace=True)

"""Tahap 10: Seleksi fitur. Dipilih lima fitur kuantitatif utama yang berhubungan langsung dengan harga saham: `Open`, `High`, `Low`, `Close`, dan `Adj Close`. Fitur `Volume` tidak digunakan"""

features = ["Open", "High", "Low", "Close", "Adj Close"]
data = df[features]

"""Tahap 11: Normalisasi data. Seluruh nilai fitur dinormalisasi ke dalam rentang [0, 1] menggunakan `MinMaxScaler` dari scikit-learn. Normalisasi diperlukan agar model pembelajaran mesin dapat memproses data secara seimbang tanpa bias terhadap fitur yang memiliki skala lebih besar"""

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

"""Tahap 12: Membuat fungsi sliding window. Fungsi `window()` dibuat untuk membentuk dataset time series dengan pendekatan sliding window. Setiap sampel input (X) terdiri dari 60 hari data sebelumnya, dan target output (y) adalah harga `Close` pada hari ke-61"""

def window(data, window_size=60):
    X, y = [], []
    for i in range(window_size, len(data)):
        X.append(data[i - window_size:i])
        y.append(data[i][3])
    return np.array(X), np.array(y)

"""Tahap 13: Membentuk data time series. Dengan menggunakan fungsi sliding window, data diubah menjadi format 3 dimensi untuk keperluan model seperti LSTM: (jumlah sampel, panjang window, jumlah fitur)"""

window_size = 60
X, y = window(scaled_data, window_size)

"""Tahap 14: Menyesuaikan tanggal dengan window. Karena data diubah menggunakan sliding window, maka indeks tanggal juga harus disesuaikan. Tanggal baru dimulai dari titik ke-61 dan akan digunakan untuk keperluan plotting hasil prediksi."""

dates = df["Date"][window_size:]
dates_train, dates_test = train_test_split(dates, test_size=0.2, shuffle=False)

"""Tahap 15: Membagi data menjadi data latih dan data uji. Data dibagi menjadi dua bagian: 80% untuk pelatihan dan 20% untuk pengujian. Pembagian dilakukan tanpa pengacakan (`shuffle=False`) karena urutan data sangat penting dalam analisis time series."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

"""### Modeling

##### LSTM

Tahap 16: Pembangunan, pelatihan, dan evaluasi model LSTM

Pada tahap ini dilakukan serangkaian proses untuk membangun dan mengevaluasi model Long Short-Term Memory (LSTM), yang dirancang khusus untuk data time series.

- **Arsitektur Model**: Model terdiri dari dua lapisan LSTM dengan masing-masing 50 unit. Lapisan pertama mengembalikan urutan agar dapat diteruskan ke lapisan berikutnya. Lapisan terakhir adalah `Dense` untuk memproduksi output berupa prediksi harga.
- **Pelatihan Model**: Model dilatih selama 20 epoch dengan batch size 32 menggunakan fungsi loss `mean_squared_error` dan optimizer `adam`. Validasi dilakukan pada data uji.
- **Prediksi dan Inverse Transform**: Setelah pelatihan, model digunakan untuk memprediksi harga `Close`. Karena data telah dinormalisasi sebelumnya, hasil prediksi dikembalikan ke skala aslinya menggunakan `MinMaxScaler`.
- **Visualisasi Hasil**: Hasil prediksi dibandingkan dengan nilai aktual dalam bentuk grafik garis terhadap waktu, untuk melihat akurasi visual dari model.
- **Evaluasi Model**: Evaluasi dilakukan menggunakan metrik regresi:
  - MAE (Mean Absolute Error)
  - MSE (Mean Squared Error)
  - RMSE (Root Mean Squared Error)
  - R2 Score

Tujuan akhir dari tahap ini adalah mengetahui seberapa baik model LSTM dalam memprediksi harga saham berdasarkan data historis.
"""

model = Sequential()
model.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(units=32))
model.add(Dense(1))
model.compile(optimizer="adam", loss="mean_squared_error")

# Latih model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Prediksi
predicted = model.predict(X_test)

# Siapkan array agar bisa di-inverse transform (jumlah kolom sama dengan data asli)
y_test_full = np.zeros((y_test.shape[0], data.shape[1]))
y_test_full[:, 1] = y_test

predicted_full = np.zeros((predicted.shape[0], data.shape[1]))
predicted_full[:, 1] = predicted.reshape(-1)

# Inverse transform
y_test_rescaled = scaler.inverse_transform(y_test_full)[:, 1]
predicted_rescaled = scaler.inverse_transform(predicted_full)[:, 1]

# Visualisasi
plt.figure(figsize=(12, 6))
plt.plot(dates_test, y_test_rescaled, color="blue", label="Harga Close Aktual")
plt.plot(dates_test, predicted_rescaled, color="red", label="Harga Close Prediksi")
plt.title("Harga Saham (ANTM.JK)")
plt.xlabel("Tanggal")
plt.ylabel("Harga")
plt.legend()
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
plt.gcf().autofmt_xdate()
plt.tight_layout()
plt.show()

# Evaluasi performa model
lstm_mae = mean_absolute_error(y_test_rescaled, predicted_rescaled)
lstm_mse = mean_squared_error(y_test_rescaled, predicted_rescaled)
lstm_rmse = np.sqrt(lstm_mse)
lstm_r2 = r2_score(y_test_rescaled, predicted_rescaled)

"""##### GRU

Tahap 17: Pembangunan, pelatihan, dan evaluasi model GRU

Pada tahap ini dilakukan proses serupa seperti LSTM, namun menggunakan model **Gated Recurrent Unit (GRU)**. GRU merupakan alternatif dari LSTM dengan struktur yang lebih sederhana dan efisien, namun tetap efektif dalam mempelajari pola dari data time series.

- **Arsitektur Model**: Model GRU terdiri dari dua lapisan GRU dengan masing-masing 50 unit. Lapisan pertama mengembalikan urutan (return_sequences=True) untuk diteruskan ke lapisan kedua. Lapisan terakhir berupa `Dense` untuk menghasilkan satu nilai prediksi.
- **Pelatihan Model**: Dilatih selama 20 epoch dengan batch size 32 menggunakan `adam` optimizer dan fungsi loss `mean_squared_error`.
- **Prediksi dan Inverse Transform**: Setelah pelatihan, dilakukan prediksi pada data uji. Hasil prediksi dikembalikan ke skala harga sebenarnya menggunakan kembali `MinMaxScaler`.
- **Visualisasi Hasil**: Dibandingkan antara harga aktual dan harga prediksi untuk menilai performa model secara visual.
- **Evaluasi Model**: Kinerja model dievaluasi dengan metrik:
  - MAE (Mean Absolute Error)
  - MSE (Mean Squared Error)
  - RMSE (Root Mean Squared Error)
  - R2 Score

Tujuan dari tahap ini adalah mengevaluasi apakah model GRU dapat menjadi alternatif yang kompetitif dalam memprediksi harga saham berdasarkan pola historis.
"""

# Model GRU
model = Sequential()
model.add(GRU(units=32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(GRU(units=32))
model.add(Dense(1))
model.compile(optimizer="adam", loss="mean_squared_error")

# Latih model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Lakukan prediksi
predicted = model.predict(X_test)

# Siapkan array agar bisa di-inverse transform (jumlah kolom sama dengan data asli)
y_test_full = np.zeros((y_test.shape[0], data.shape[1]))
y_test_full[:, 1] = y_test

predicted_full = np.zeros((predicted.shape[0], data.shape[1]))
predicted_full[:, 1] = predicted.reshape(-1)

# Inverse transform
y_test_rescaled = scaler.inverse_transform(y_test_full)[:, 1]
predicted_rescaled = scaler.inverse_transform(predicted_full)[:, 1]

# Visualisasi
plt.figure(figsize=(12, 6))
plt.plot(dates_test, y_test_rescaled, color="blue", label="Harga Close Aktual")
plt.plot(dates_test, predicted_rescaled, color="red", label="Harga Close Prediksi")
plt.title("Harga Saham (ANTM.JK)")
plt.xlabel("Tanggal")
plt.ylabel("Harga")
plt.legend()
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
plt.gcf().autofmt_xdate()
plt.tight_layout()
plt.show()

# Evaluasi
gru_mae = mean_absolute_error(y_test_rescaled, predicted_rescaled)
gru_mse = mean_squared_error(y_test_rescaled, predicted_rescaled)
gru_rmse = np.sqrt(gru_mse)
gru_r2 = r2_score(y_test_rescaled, predicted_rescaled)

"""#### Hyperparameter Tuning

Tahap 18: Pembuatan Fungsi Model dan Penyiapan Hyperparameter Tuning

Pada tahap ini, dibuat fungsi untuk mendefinisikan arsitektur model **LSTM** dan **GRU** agar dapat digunakan bersama dengan `KerasRegressor`. Hal ini memudahkan dalam melakukan **hyperparameter tuning** menggunakan `GridSearchCV`.

Penjelasan Proses:
- **Fungsi `lstm_model` dan `gru_model`**:
  - Menerima parameter `units` dan `optimizer` untuk fleksibilitas tuning.
  - Arsitektur terdiri dari dua layer LSTM atau GRU dan satu output `Dense`.
  - Kompilasi model menggunakan `mean_squared_error` sebagai fungsi loss, sesuai untuk regresi.
  - GRU model menambahkan metrik `mae` untuk evaluasi tambahan.

- **KerasRegressor**:
  - `KerasRegressor` membungkus fungsi model agar bisa digunakan seperti model scikit-learn.
  - Parameter tuning dilakukan dengan `verbose=1` untuk menampilkan progres pelatihan.

- **Grid Search Setup**:
  - Didefinisikan dictionary `param_grid` untuk eksplorasi beberapa kombinasi parameter:
    - `units`: Jumlah unit pada masing-masing layer (32, 50, 64).
    - `batch_size`: Ukuran batch pelatihan (16, 32).
    - `epochs`: Jumlah epoch (10, 20).
    - `optimizer`: Optimizer yang digunakan (`adam`, `rmsprop`).
    
Grid Search ini memungkinkan pencarian konfigurasi model yang paling optimal berdasarkan performa validasi.
"""

# Model LSTM
def lstm_model(units=50, optimizer="adam"):
    model = Sequential()
    model.add(LSTM(units=units, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(LSTM(units=units))
    model.add(Dense(1))
    model.compile(loss="mean_squared_error", optimizer=optimizer)
    return model

lstm_reg = KerasRegressor(build_fn=lstm_model, verbose=1)

# Model GRU
def gru_model(units=50, optimizer="adam"):
    model = Sequential()
    model.add(GRU(units=units, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(GRU(units=units))
    model.add(Dense(1))
    model.compile(loss="mean_squared_error", optimizer=optimizer, metrics=["mae"])
    return model

gru_reg = KerasRegressor(build_fn=gru_model, verbose=1)

# Parameter untuk di lakukan tuning
param_grid = {
    "units": [32, 50, 64],
    "batch_size": [16, 32],
    "epochs": [10, 20],
    "optimizer": ["adam", "rmsprop"]
}

"""##### LSTM

Tahap 19: Hyperparameter tuning model LSTM menggunakan Randomized Search

Pada tahap ini dilakukan pencarian kombinasi hyperparameter terbaik untuk model LSTM menggunakan **RandomizedSearchCV** dari scikit-learn.

Penjelasan Proses:
- **RandomizedSearchCV**:
  - Berbeda dengan `GridSearchCV` yang mencoba semua kombinasi, `RandomizedSearchCV` memilih sejumlah kombinasi parameter secara acak berdasarkan distribusi yang diberikan. Hal ini mempercepat proses pencarian, terutama jika ruang pencarian besar.
  - Parameter yang digunakan:
    - `estimator`: Objek `KerasRegressor` berbasis model LSTM.
    - `param_distributions`: Dictionary `param_grid` berisi kombinasi parameter yang akan diuji.
    - `n_iter=5`: Hanya mencoba 5 kombinasi acak dari grid parameter.
    - `cv=3`: Melakukan cross-validation sebanyak 3 fold untuk mengevaluasi setiap kombinasi.
    - `random_state=42`: Untuk memastikan hasil acak yang konsisten.

- **Hasil Akhir**:
  - Setelah proses tuning, `best_params_` akan menampilkan kombinasi parameter yang menghasilkan performa terbaik berdasarkan hasil validasi silang.

Langkah ini sangat penting untuk memastikan model LSTM bekerja secara optimal tanpa melakukan eksplorasi parameter secara manual.
"""

# Random Search LSTM
random_search = RandomizedSearchCV(estimator=lstm_reg, param_distributions=param_grid, n_iter=5, cv=3, random_state=42)
random_search_result = random_search.fit(X_train, y_train)
print("Parameter terbaik dari Random Search:", random_search_result.best_params_)

"""Tahap 20: Pelatihan Ulang dan Evaluasi Model LSTM dengan Parameter Terbaik dari Randomized Search

Setelah mendapatkan parameter terbaik dari hasil tuning, tahap ini meliputi:

- **Membangun ulang model LSTM** menggunakan parameter terbaik seperti jumlah unit dan optimizer.
- **Melatih model** kembali dengan jumlah epoch dan batch size sesuai hasil tuning.
- **Melakukan prediksi** harga saham pada data uji.
- **Inverse transform** hasil prediksi dan nilai aktual ke skala harga asli agar dapat dibandingkan secara langsung.
- **Visualisasi** hasil prediksi versus harga aktual dalam grafik time series untuk analisis performa secara visual.
- **Evaluasi model** menggunakan metrik regresi (MAE, MSE, RMSE, R2) untuk kuantifikasi akurasi prediksi.

Langkah ini memastikan model menggunakan konfigurasi hyperparameter terbaik yang ditemukan dan mengevaluasi performanya secara menyeluruh.

"""

# Simpan parameter terbaik
best_model = lstm_model(
    units=random_search_result.best_params_["units"],
    optimizer=random_search_result.best_params_["optimizer"],
)

# Latih model dengan parameter sebelumnya
history = best_model.fit(
    X_train, y_train,
    epochs=random_search_result.best_params_["epochs"],
    batch_size=random_search_result.best_params_["batch_size"],
    validation_data=(X_test, y_test)
)

# Lakukan prediksi
predicted = best_model.predict(X_test)

# Siapkan array agar bisa di-inverse transform (jumlah kolom sama dengan data asli)
y_test_full = np.zeros((y_test.shape[0], data.shape[1]))
y_test_full[:, 1] = y_test

predicted_full = np.zeros((predicted.shape[0], data.shape[1]))
predicted_full[:, 1] = predicted.reshape(-1)

# Inverse transform
y_test_rescaled = scaler.inverse_transform(y_test_full)[:, 1]
predicted_rescaled = scaler.inverse_transform(predicted_full)[:, 1]

# Visualisasi
plt.figure(figsize=(12, 6))
plt.plot(dates_test, y_test_rescaled, color="blue", label="Harga Close Aktual")
plt.plot(dates_test, predicted_rescaled, color="red", label="Harga Close Prediksi")
plt.title("Harga Saham (ANTM.JK)")
plt.xlabel("Tanggal")
plt.ylabel("Harga")
plt.legend()
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
plt.gcf().autofmt_xdate()
plt.tight_layout()
plt.show()

# Evaluasi
lstm_mae_rs = mean_absolute_error(y_test_rescaled, predicted_rescaled)
lstm_mse_rs = mean_squared_error(y_test_rescaled, predicted_rescaled)
lstm_rmse_rs = np.sqrt(lstm_mse_rs)
lstm_r2_rs = r2_score(y_test_rescaled, predicted_rescaled)

"""##### GRU

Tahap 21: Hyperparameter Tuning Model GRU Menggunakan Randomized Search

Pada tahap ini dilakukan pencarian parameter terbaik untuk model GRU menggunakan `RandomizedSearchCV` yang menguji kombinasi parameter secara acak.

Penjelasan proses:
- `estimator`: Model GRU yang dibungkus dengan `KerasRegressor`.
- `param_distributions`: Dictionary parameter `param_grid` yang berisi nilai-nilai kandidat.
- `n_iter=5`: Mengeksekusi 5 kombinasi parameter acak.
- `cv=3`: Melakukan validasi silang sebanyak 3 kali untuk setiap kombinasi.
- `random_state=42`: Supaya hasil pencarian dapat direproduksi.

Proses ini membantu menemukan konfigurasi terbaik untuk arsitektur dan pelatihan model GRU tanpa harus mencoba semua kemungkinan secara manual.
"""

# Random Search GRU
random_search = RandomizedSearchCV(estimator=gru_reg, param_distributions=param_grid, n_iter=5, cv=3, random_state=42)
random_search_result = random_search.fit(X_train, y_train)
print("Parameter terbaik dari Random Search:", random_search_result.best_params_)

"""Tahap 22: Pelatihan ulang dan evaluasi model GRU dengan parameter terbaik dari Randomized Search

Setelah mendapatkan parameter terbaik dari tuning, dilakukan langkah berikut:

- **Membangun ulang model GRU** dengan jumlah unit dan optimizer sesuai parameter hasil tuning.
- **Melatih model kembali** menggunakan epoch dan batch size yang optimal.
- **Melakukan prediksi** pada data uji.
- **Inverse transform** prediksi dan data aktual agar dapat dibandingkan dalam skala asli harga saham.
- **Visualisasi** hasil prediksi dan harga aktual pada grafik time series.
- **Evaluasi** performa menggunakan metrik MAE, MSE, RMSE, dan R2 sebagai ukuran akurasi prediksi.

Langkah ini memastikan model GRU memakai konfigurasi hyperparameter terbaik dan memberikan gambaran performa yang jelas.

"""

# Simpan parameter terbaik
best_model = gru_model(
    units=random_search_result.best_params_["units"],
    optimizer=random_search_result.best_params_["optimizer"]
)

# Latih model dengan parameter sebelumnya
history = best_model.fit(
    X_train, y_train,
    epochs=random_search_result.best_params_["epochs"],
    batch_size=random_search_result.best_params_["batch_size"],
    validation_data=(X_test, y_test)
)

# Lakukan prediksi
predicted = best_model.predict(X_test)

# Siapkan array agar bisa di-inverse transform (jumlah kolom sama dengan data asli)
y_test_full = np.zeros((y_test.shape[0], data.shape[1]))
y_test_full[:, 1] = y_test

predicted_full = np.zeros((predicted.shape[0], data.shape[1]))
predicted_full[:, 1] = predicted.reshape(-1)

# Inverse transform
y_test_rescaled = scaler.inverse_transform(y_test_full)[:, 1]
predicted_rescaled = scaler.inverse_transform(predicted_full)[:, 1]

# Visualisasi
plt.figure(figsize=(12, 6))
plt.plot(dates_test, y_test_rescaled, color="blue", label="Harga Close Aktual")
plt.plot(dates_test, predicted_rescaled, color="red", label="Harga Close Prediksi")
plt.title("Harga Saham (ANTM.JK)")
plt.xlabel("Tanggal")
plt.ylabel("Harga")
plt.legend()
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
plt.gcf().autofmt_xdate()
plt.tight_layout()
plt.show()

# Evaluasi
gru_mae_rs = mean_absolute_error(y_test_rescaled, predicted_rescaled)
gru_mse_rs = mean_squared_error(y_test_rescaled, predicted_rescaled)
gru_rmse_rs = np.sqrt(gru_mse_rs)
gru_r2_rs = r2_score(y_test_rescaled, predicted_rescaled)

"""### Evaluasi

Tahap 23: Perbandingan hasil evaluasi model LSTM dan GRU

Pada tahap ini, dilakukan perbandingan hasil metrik evaluasi utama dari keempat skenario model, yaitu:

1. Model LSTM standar tanpa tuning parameter.
2. Model LSTM dengan parameter terbaik hasil Randomized Search.
3. Model GRU standar tanpa tuning parameter.
4. Model GRU dengan parameter terbaik hasil Randomized Search.

Metrik evaluasi yang diprint meliputi:
- Mean Absolute Error (MAE)
- Mean Squared Error (MSE)
- Root Mean Squared Error (RMSE)
- R2 Score

Perbandingan ini membantu menentukan model dan konfigurasi mana yang memberikan hasil prediksi terbaik untuk harga saham ANTM.JK.
"""

# Print hasil evaluasi semuanya
print("Evaluasi LSTM:")
print(f"Mean Absolute Error (MAE): {lstm_mae:.2f}")
print(f"Mean Squared Error (MSE): {lstm_mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {lstm_rmse:.2f}")
print(f"R2 Score: {lstm_r2:.4f}")

print("\nEvaluasi LSTM (Random Search):")
print(f"Mean Absolute Error (MAE): {lstm_mae_rs:.2f}")
print(f"Mean Squared Error (MSE): {lstm_mse_rs:.2f}")
print(f"Root Mean Squared Error (RMSE): {lstm_rmse_rs:.2f}")
print(f"R2 Score: {lstm_r2_rs:.4f}")

print("\nEvaluasi GRU:")
print(f"Mean Absolute Error (MAE): {gru_mae:.2f}")
print(f"Mean Squared Error (MSE): {gru_mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {gru_rmse:.2f}")
print(f"R2 Score: {gru_r2:.4f}")

print("\nEvaluasi GRU (Random Search):")
print(f"Mean Absolute Error (MAE): {gru_mae_rs:.2f}")
print(f"Mean Squared Error (MSE): {gru_mse_rs:.2f}")
print(f"Root Mean Squared Error (RMSE): {gru_rmse_rs:.2f}")
print(f"R2 Score: {gru_r2_rs:.4f}")